<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>How It Works · Reflective Diagnostics</title>
  <meta name="description" content="Replayable measurement pipeline with deterministic artifacts and an audit-grade evidence packet." />
  <link rel="stylesheet" href="assets/site.css" />
</head>

<body>
  <!-- Top Nav -->
  <div class="topbar">
    <div class="topbar-inner">
      <div class="brand">
        <div class="t">Reflective Diagnostics™</div>
        <div class="s">Structural Reasoning Risk · restricted evaluation · auditable diagnostics</div>
      </div>

      <div class="nav">
        <a href="index.html">Start Here</a>
        <a class="active" href="how-it-works.html">How It Works</a>
        <a href="what-it-measures.html">What It Measures</a>
        <a href="use-cases.html">Use Cases and Value</a>
        <a href="evidence.html">Proof and Evidence</a>
        <a href="funding-roadmap.html">Funding and Roadmap</a>
        <a href="compliance.html">Compliance</a>
      </div>
    </div>
  </div>

  <div class="wrap">
    <!-- HERO -->
    <section class="hero">
      <div class="kickers">
        <div class="pill accent">Research Prototype v1.0</div>
        <div class="pill">Deterministic Artifacts</div>
        <div class="pill">Cross Model Comparison</div>
        <div class="pill">Evidence Locked</div>

        <!-- keep this exact style: black badge + green -->
        <div class="badge-verified" title="This page is a public record of the evaluation site and evidence chain.">
          <span class="dot"></span>
          <span>Integrity Verified</span>
        </div>
      </div>

      <h1>How it works: a replayable measurement pipeline (not “LLM as judge”).</h1>

      <p class="subtitle">
        We measure one thing: whether conclusions are <b>structurally supported</b> by stated premises.
        Every run produces a <b>replayable evidence packet</b>: raw outputs + metadata + hashes + extracted structure + computed signals.
      </p>

      <div class="btnrow">
        <a class="btn primary" href="evidence.html">Open Proof and Evidence</a>
        <a class="btn" href="what-it-measures.html">Read Metric Definitions</a>
        <a class="btn" href="compliance.html">See Regime Mapping (Compliance)</a>
      </div>
    </section>

    <!-- PIPELINE -->
    <section class="grid">
      <div class="panel">
        <h2>End to End Pipeline</h2>

        <div class="pipeline-line mono">
          INPUT → CONTROLLED RUN → ARTIFACT CAPTURE → STRUCTURAL MAP → SIGNALS → CROSS MODEL SYNTHESIS → EVIDENCE PACKET
        </div>

        <div class="small">
          Each stage is explicit. Outputs from a stage become fixed inputs to the next stage.
        </div>
      </div>
    </section>

    <!-- 4 STAGES (EVALUATOR READABLE) -->
    <section class="grid two">
      <div class="panel">
        <h2>1) Controlled Run (comparability)</h2>
        <ul>
          <li>Same input, same constraints, across models</li>
          <li>No hidden retries, no prompt rewriting, no adaptive steering</li>
          <li>Model identity and version recorded</li>
        </ul>
        <div class="accent-note">
          Goal: differences observed are attributable to the model, not shifting conditions.
        </div>
      </div>

      <div class="panel">
        <h2>2) Artifact Capture (source of record)</h2>
        <ul>
          <li>Raw model outputs stored verbatim</li>
          <li>Run metadata captured (timestamps, run IDs, selection)</li>
          <li>Hashes generated to prevent silent alteration</li>
        </ul>
        <div class="accent-note">
          Result: a run can be audited by someone who did not witness it.
        </div>
      </div>
    </section>

    <section class="grid two">
      <div class="panel">
        <h2>3) Structural Map (inspectable support graph)</h2>
        <ul>
          <li>Extract claims, premises, and conclusions</li>
          <li>Compute linkage: what supports what</li>
          <li>Expose unsupported commitments (orphans)</li>
        </ul>
        <div class="accent-note">
          Output: an inspectable structure, not a “vibe” score.
        </div>
      </div>

      <div class="panel">
        <h2>4) Signals + Synthesis (cross model)</h2>
        <ul>
          <li>Compute structural signals (coverage, orphans, integrity, stability)</li>
          <li>Rank and compare models under identical constraints</li>
          <li>Surface divergence: where models disagree structurally</li>
        </ul>
        <div class="accent-note">
          Output: a cross model decision record tied to artifacts.
        </div>
      </div>
    </section>

    <!-- WHAT MAKES THIS NOT LLM AS JUDGE -->
    <section class="grid">
      <div class="panel">
        <h2>Why this is not “LLM as judge”</h2>
        <ul>
          <li>No model scores itself</li>
          <li>No subjective rubric or preference voting</li>
          <li>Signals are derived from extracted structure + fixed artifacts</li>
        </ul>
      </div>
    </section>

    <!-- EVIDENCE PACKET -->
    <section class="grid">
      <div class="panel">
        <h2>What an evaluator can export</h2>
        <ul>
          <li>Raw outputs (verbatim)</li>
          <li>Run metadata + hashes (provenance)</li>
          <li>Structural map (claims / premises / conclusions)</li>
          <li>Signals + cross model synthesis (decision exhibit)</li>
        </ul>

        <div class="callout">
          The key deliverable is the <b>evidence packet</b>: a replayable case file suitable for audit and governance review.
        </div>
      </div>
    </section>

    <div class="footer">
      <div>Contact: <span class="mono">research@enlightenedai.ai</span></div>
      <div class="mono">Build: Research Prototype v1.0 · Reflective Diagnostics™ · restricted evaluation</div>
    </div>
  </div>
</body>
</html>
