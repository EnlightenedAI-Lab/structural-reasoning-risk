<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Reflective Diagnostics — Structural Reasoning Risk</title>
  <meta name="description" content="Decision-grade structural reasoning diagnostics with auditable traceability." />
  <link rel="stylesheet" href="assets/site.css" />
</head>

<body>
  <!-- Top Nav -->
  <div class="topbar">
    <div class="topbar-inner">
      <div class="brand">
        <div class="t">Reflective Diagnostics™</div>
        <div class="s">Structural Reasoning Risk · restricted evaluation · auditable diagnostics</div>
      </div>

      <div class="nav">
        <a class="active" href="index.html">Start Here</a>
        <a href="how-it-works.html">How It Works</a>
        <a href="what-it-measures.html">What It Measures</a>
        <a href="use-cases.html">Use Cases and Value</a>
        <a href="evidence.html">Proof and Evidence</a>
        <a href="funding-roadmap.html">Funding and Roadmap</a>
        <a href="compliance.html">Compliance</a>
      </div>
    </div>
  </div>

  <div class="wrap">
    <!-- HERO -->
    <section class="hero">
      <div class="kickers">
        <div class="pill accent">Research Prototype v1.0</div>
        <div class="pill">Audit Trail Enforced</div>
        <div class="pill">Cross Model Comparison</div>
        <div class="pill">Deterministic Artifacts</div>
        <div class="badge-verified" title="This page is a public record of the evaluation site and evidence chain.">
          <span class="dot"></span>
          <span>Integrity Verified</span>
        </div>
      </div>

      <h1>Measure whether AI conclusions are structurally justified with an audit trail.</h1>
      <p class="subtitle">
        This instrument evaluates whether conclusions produced by AI systems are <b>derivable from their stated premises</b>.
        It does <b>not</b> judge factual truth, sentiment, or policy merit. It measures structural justification and exposes
        unsupported commitments before high stakes decisions are made.
      </p>

      <div class="btnrow">
        <a class="btn primary" href="evidence.html">Open Proof and Evidence</a>
        <a class="btn" href="use-cases.html">Use Cases and Value</a>
        <a class="btn" href="funding-roadmap.html">Funding and Roadmap</a>
      </div>
    </section>

    <!-- CORE PROBLEM / GAP -->
    <section class="grid two">
      <div class="card">
        <h2>The Problem</h2>
        <ul>
          <li>AI generated analyses are increasingly used in funding, policy, compliance, and governance workflows.</li>
          <li>Outputs can sound coherent while being structurally unsupported, with missing premises and orphan conclusions.</li>
          <li>Human review is inconsistent and costly. Failures are often discovered after commitments are made.</li>
        </ul>
        <div class="callout">
          The core risk is unsupported commitment packaged as reasoning.
        </div>
      </div>

      <div class="card">
        <h2>What Existing Tools Miss</h2>
        <ul>
          <li>Benchmarks focus on accuracy. They do not verify whether a conclusion follows from provided premises.</li>
          <li>Red teaming finds edge cases but does not produce audit grade, replayable reasoning structure.</li>
          <li>Preference and rubric scoring evaluates outputs, not the internal support structure of claims.</li>
        </ul>
        <div class="callout">
          This instrument measures structure: claim premise linkage, coverage, orphans, stability, and cross model divergence.
        </div>
      </div>
    </section>

    <!-- WHAT IT IS / WHO IT SERVES / AUTHORITY -->
    <section class="grid three">
      <div class="card">
        <h2>What This Is</h2>
        <p>
          A decision grade measurement instrument that converts AI generated analysis into an inspectable structure
          (claims, premises, conclusions) and quantifies structural integrity with replayable evidence artifacts.
        </p>
        <p class="mono">Non goal: truth verification. Goal: structural justification and auditability.</p>
      </div>

      <div class="card">
        <h2>Who It Serves</h2>
        <ul>
          <li>Public funders and program evaluators</li>
          <li>Regulators, auditors, and compliance teams</li>
          <li>Enterprises governing internal AI usage</li>
          <li>AI labs and evaluators comparing model reliability</li>
        </ul>
      </div>

      <div class="card">
        <h2>Authority It Provides</h2>
        <p>
          The ability to state, with a traceable record, whether an AI generated conclusion is structurally supported,
          and to compare models under identical constraints.
        </p>
        <div class="callout">
          In audits and governance reviews: we can show the support chain, not just the output.
        </div>
      </div>
    </section>

    <!-- WHERE TO GO NEXT -->
    <section class="grid">
      <div class="card">
        <h2>Where to Start</h2>
        <ul>
          <li><b>How It Works</b> shows the pipeline from probe to artifacts to metrics to interpreter.</li>
          <li><b>What It Measures</b> defines core signals: coverage, orphans, stability, integrity.</li>
          <li><b>Use Cases and Value</b> maps decision contexts, cost avoidance, and adoption logic.</li>
          <li><b>Proof and Evidence</b> contains the 10 screenshot evidence chain demonstrating the live instrument.</li>
          <li><b>Funding and Roadmap</b> specifies what funding unlocks next: validation, regimes, reporting, deployment hardening.</li>
          <li><b>Compliance</b> is the specification for the next module, translating structural signals into regime constraints.</li>
        </ul>
      </div>
    </section>

    <div class="footer">
      <div>Contact: <span class="mono">research@enlightenedai.ai</span></div>
      <div class="mono">Build: Research Prototype v1.0 · Reflective Diagnostics™ · restricted evaluation</div>
    </div>
  </div>
</body>
</html>
