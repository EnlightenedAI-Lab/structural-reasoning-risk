<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Reflective Diagnostics™</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="styles.css" />
</head>

<body>

<header class="site-header">
  <div class="brand">
    <h1>Reflective Diagnostics™</h1>
    <p class="subtitle">Structural Reasoning Risk · restricted evaluation · auditable diagnostics</p>
  </div>

  <nav class="main-nav">
    <a href="#start" class="active">Start Here</a>
    <a href="#how">How It Works</a>
    <a href="#measures">What It Measures</a>
    <a href="#usecases">Use Cases and Value</a>
    <a href="#proof">Proof and Evidence</a>
    <a href="#funding">Funding and Roadmap</a>
    <a href="#compliance">Compliance</a>
  </nav>
</header>

<section class="badges">
  <span class="badge dark">Research Prototype v1.0</span>
  <span class="badge">Audit Trail Enforced</span>
  <span class="badge">Cross-Model Comparison</span>
  <span class="badge">Deterministic Artifacts</span>
</section>

<main>

<section class="hero" id="start">
  <h2>
    An audit instrument that compares AI systems by the structural justification of their conclusions,
    exposing unsupported commitments before high-stakes decisions are made.
  </h2>
</section>

<section class="cards">

  <div class="card">
    <h3>THE PROBLEM</h3>
    <ul>
      <li>AI outputs are used to justify funding, policy, compliance, and governance decisions</li>
      <li>Evaluation relies on trust in model providers, not verifiable reasoning structure</li>
      <li>Models change continuously, making oversight unstable over time</li>
      <li>The ecosystem is fragmented across vendors, versions, and deployment contexts</li>
      <li>Coherent outputs can introduce commitments no human explicitly approved</li>
    </ul>
    <p class="core-risk">
      <strong>The core risk:</strong><br />
      Unsupported conclusions are treated as decisions without a reliable way to audit or compare them.
    </p>
  </div>

  <div class="card">
    <h3>WHAT EXISTING TOOLS MISS</h3>
    <ul>
      <li>Benchmarks measure answer quality, not whether conclusions follow from stated premises</li>
      <li>Red-teaming surfaces failures but does not enable repeatable, system-wide comparison</li>
      <li>Output scoring judges fluency and alignment, not internal support structure</li>
      <li>Safety layers limit content categories, not unjustified commitments</li>
      <li>No tool establishes a stable standard across models, updates, and providers</li>
    </ul>
    <p class="core-risk">
      <strong>The gap:</strong><br />
      Existing tools evaluate performance and safety outcomes, but cannot determine whether an AI system’s
      conclusions are structurally justified or comparable across models.
    </p>
  </div>

</section>

<section class="card single">
  <h3>WHAT THIS INSTRUMENT PROVIDES</h3>
  <p>
    A replayable audit that reveals when AI conclusions exceed their evidence.
  </p>
</section>

<section class="cards">

  <div class="card">
    <h3>WHO IT SERVES</h3>
    <ul>
      <li><strong>Decision authorities</strong> — funders, regulators, and auditors responsible for consequential outcomes</li>
      <li><strong>Operators</strong> — enterprises deploying AI in compliance-sensitive environments</li>
      <li><strong>Builders</strong> — AI labs requiring auditable proof of reasoning integrity</li>
    </ul>
  </div>

  <div class="card">
    <h3>AUTHORITY IT PROVIDES</h3>
    <p>
      Objective justification of AI conclusions through traceable support structure.
    </p>
  </div>

</section>

</main>

<footer class="site-footer">
  <p>© Reflective Diagnostics™ — Research Prototype</p>
</footer>

</body>
</html>
